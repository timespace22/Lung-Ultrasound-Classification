{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d37527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 # random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b965ce",
   "metadata": {},
   "source": [
    "## Loading the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7983dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_info = pd.read_excel(\"Lung_level_final_table.xlsx\") # Table with Patient, Video ID, DiseaseID, and the link to the Video File\n",
    "table_info = table_info.rename(columns = {'ID':'Paciente ',\"Diseased (Normal : 0 Abnormal : 1)\":\"DiseaseID\"})\n",
    "table_info['patient_video'] = table_info['Paciente '] + \"_\" + table_info['Video ID']\n",
    "\n",
    "# MAX BRIGHTNESS FRAMES\n",
    "frames_50_folder = \"Final_Frames_from_Videos_cons50/\"\n",
    "frames_100_folder = \"Final_Frames_from_Videos_cons100/\"\n",
    "\n",
    "# MIDDLE 50% FRAMES\n",
    "frames_middle_50_skip_0_folder = \"Frames_from_Videos_middle_50_skip_0/\"\n",
    "frames_middle_50_skip_1_folder = \"Frames_from_Videos_middle_50_skip_1/\"\n",
    "frames_middle_50_skip_2_folder = \"Frames_from_Videos_middle_50_skip_2/\"\n",
    "\n",
    "table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_info['DiseaseID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ec043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "frame_folder = frames_middle_50_skip_0_folder # use the required frame folder\n",
    "patient_video_list = os.listdir(frame_folder) \n",
    "file_path = []\n",
    "paciente = []\n",
    "video_id = []\n",
    "\n",
    "# Create a mapping of values to their corresponding order\n",
    "mapping = {value: index for index, value in enumerate(patient_video_list)}\n",
    "\n",
    "# Apply the mapping to the DataFrame's column\n",
    "table_info['patient_video'] = table_info['patient_video'].map(mapping)\n",
    "\n",
    "# Sort the DataFrame based on the mapped values\n",
    "table_info = table_info.sort_values(by='patient_video')\n",
    "\n",
    "# Reset the index to maintain a clean index order\n",
    "table_info = table_info.reset_index(drop=True)\n",
    "\n",
    "table_info['patient_video'] = table_info['Paciente '] + \"_\" + table_info['Video ID']\n",
    "table_info\n",
    "\n",
    "for patient_video in patient_video_list:\n",
    "    frame_list = os.listdir(frame_folder + patient_video + '/')\n",
    "    for frame in frame_list:\n",
    "        paciente.append(patient_video.split(\"_\")[0])\n",
    "        video_id.append(patient_video.split(\"_\")[1])\n",
    "        file_path.append(frame_folder + patient_video + '/' + frame)\n",
    "        \n",
    "    \n",
    "train = pd.DataFrame({\"Paciente \":paciente, \"Video ID\": video_id, \"File\": file_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e494e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of patient-videoid to the frame image location\n",
    "train = pd.merge(train, table_info[['Paciente ','Video ID','DiseaseID']], on = ['Paciente ','Video ID'], how = 'inner')\n",
    "train['key'] = train['Paciente '] + train['Video ID']\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of normal and abnormal frames\n",
    "train['DiseaseID'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3f46c",
   "metadata": {},
   "source": [
    "## Train test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE =128 # fixing image size to 128 * 128 * 1\n",
    "data_dir = os.getcwd()\n",
    "\n",
    "def read_image(filepath):\n",
    "    return cv2.imread(os.path.join(data_dir, filepath), cv2.IMREAD_GRAYSCALE) # READING AS GRAYSCALE TO KEEP SINGLE CHANNEL\n",
    "\n",
    "# Resize image to target size\n",
    "def resize_image(image, image_size):\n",
    "    resized_image = cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)\n",
    "    return resized_image[:, :, np.newaxis]\n",
    "\n",
    "X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "\n",
    "for i, file in tqdm(enumerate(train['File'].values)):\n",
    "    image = read_image(file)\n",
    "    image = image[:, :, np.newaxis]\n",
    "\n",
    "    if image is not None:\n",
    "        X_train[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    \n",
    "\n",
    "X_Train = X_train / 255.\n",
    "print('Train Shape: {}'.format(X_Train.shape))\n",
    "\n",
    "Y_Train = train['DiseaseID'].values\n",
    "print('Total target labels:',len(Y_Train))\n",
    "\n",
    "Y_Train = to_categorical(Y_Train, num_classes=2) # num_classes: number of classes in target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487973ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_videoids = train[['key']].groupby(['key']).sum().reset_index()\n",
    "\n",
    "key_split = patient_videoids.iloc[int(np.floor(len(patient_videoids)*0.7)) - 1]['key']\n",
    "train_len = max(train[(train['key'] == key_split)].index) + 1\n",
    "\n",
    "X_train = X_Train[:train_len]\n",
    "X_val = X_Train[train_len:]\n",
    "Y_train = Y_Train[:train_len]\n",
    "Y_val = Y_Train[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d14501",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_table = train.copy()\n",
    "split_table['Split'] = \"test\"\n",
    "split_table['Split'].iloc[:train_len] = \"train\"\n",
    "\n",
    "split_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81326c75",
   "metadata": {},
   "source": [
    "##### Note: \"train_len\" variable will be used in many calculations later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66f002",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required functions\n",
    "from keras.layers import Input, Conv2D, concatenate, MaxPool2D, Dropout, Dense, Activation, Flatten, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.applications.densenet import DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image input initialization\n",
    "\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 1\n",
    " \n",
    "inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network architecture\n",
    "s=inputs\n",
    "\n",
    "# 1st branch\n",
    "c1 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "p1=  MaxPool2D(pool_size=(2,2))(c1)\n",
    "p1=  Dropout(0.2)(p1)\n",
    "\n",
    "mid1 = p1\n",
    "\n",
    "c1_1= Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "\n",
    "# 2nd branch\n",
    "c2=  Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "p2=   MaxPool2D(pool_size=(2,2))(c2)\n",
    "\n",
    "mid2 = p2\n",
    "mid2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(mid2)\n",
    "mid2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(mid2)\n",
    "mid2 =  Dropout(0.2)(mid2)\n",
    "P1_R = MaxPool2D(pool_size=(2,2))(mid2)\n",
    "\n",
    "# concatenating from 1st and 2nd branch              \n",
    "R1=concatenate([c1_1,p2])\n",
    "\n",
    "C1_R=Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(R1)\n",
    "mid1 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(mid1)\n",
    "mid1 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(mid1)\n",
    "mid1 =  Dropout(0.2)(mid1)\n",
    "            \n",
    "mid1_1 = concatenate([C1_R,mid1])\n",
    "\n",
    "C11_R=Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(mid1_1)\n",
    "C11_R=Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(C11_R)\n",
    "C11_R = MaxPool2D(pool_size=(2,2))(C11_R)\n",
    "C11_R =  Dropout(0.2)(C11_R)\n",
    "\n",
    "mid2_1 = concatenate([C11_R,P1_R])\n",
    "\n",
    "mid2_1 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(mid2_1)\n",
    "x = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(mid2_1)\n",
    "\n",
    "densenet = DenseNet201(weights='imagenet', include_top=False)\n",
    "\n",
    "x = (Flatten())(x)\n",
    "\n",
    "# densenet branch\n",
    "c_b_1=Conv2D(3, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "\n",
    "branch_2 = densenet(c_b_1)\n",
    "    \n",
    "branch_2 = GlobalAveragePooling2D()(branch_2)\n",
    "branch_2= BatchNormalization()(branch_2)\n",
    "branch_2 = Dropout(0.5)(branch_2)\n",
    "branch_2= Dense(256, activation='relu')(branch_2)\n",
    "\n",
    "final=concatenate([x,branch_2])\n",
    "final = BatchNormalization()(final)\n",
    "final = Dropout(0.2)(final)\n",
    "final = Dense(1024, activation='relu')(final)\n",
    "final= Dropout(0.2)(final)\n",
    "final= Dense(512, activation='relu')(final)\n",
    "final= Dropout(0.2)(final) \n",
    "final= Dense(128, activation='relu')(final)\n",
    "final= Dropout(0.5)(final) \n",
    "final= Dense(64, activation='relu')(final)\n",
    "final= Dropout(0.5)(final) \n",
    "\n",
    "output = Dense(2,activation = 'softmax', name='root')(final)\n",
    "      \n",
    "model = Model(inputs,output)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.1, weight_decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "EPOCHS=15\n",
    "BATCH_SIZE=16\n",
    "\n",
    "model.fit(X_train,Y_train,batch_size=BATCH_SIZE,steps_per_epoch=X_train.shape[0] // BATCH_SIZE, epochs=EPOCHS)\n",
    "model.save(\"model_weights/model_CNN_16batch_middle50skip0_grey.h5\") # model pickle file name\n",
    "\n",
    "print((time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0be571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "## Load the corresponding model pickle file\n",
    "# model = load_model('model_weights/model_CNN_16batch_middle50skip2_grey.h5') # middle 50% skip 2 - 3.41 hours, accuracy: 0.9978\n",
    "# model = load_model('model_weights/model_CNN_16batch_middle50skip1_grey.h5') # middle 50% skip 1 - 4.61 hours, accuracy: 0.9965\n",
    "model = load_model('model_weights/model_CNN_16batch_middle50skip0_grey.h5') # middle 50% skip 0 - 11.48 hours, accuracy: 0.9996\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Testing\n",
    "Y_pred = model.predict(X_val)\n",
    "\n",
    "Y_pred_labels = np.array([round(x[1]) for x in Y_pred])\n",
    "Y_val_labels = train['DiseaseID'].values[train_len:]\n",
    "\n",
    "print((time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d5c07",
   "metadata": {},
   "source": [
    "## Calculating metrics at Frame level for CNN + DenseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Frames_list = []\n",
    "test_accuracy_list = []\n",
    "test_precision_score_list = []\n",
    "test_recall_score_list = []\n",
    "test_f1_score_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3409c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "\n",
    "Model_Frames = \"CNN Model with middle 50% frames skip 0 grey\"\n",
    "Model_Frames_list.append(Model_Frames)\n",
    "\n",
    "test_accuracy = accuracy_score(Y_val_labels, Y_pred_labels)\n",
    "test_accuracy_list.append(test_accuracy)\n",
    "print(f\"Test Accuracy from CNN using whole data as test: {test_accuracy*100}%\")\n",
    "\n",
    "test_precision_score = precision_score(Y_val_labels, Y_pred_labels)\n",
    "test_precision_score_list.append(test_precision_score)\n",
    "print(f\"Test Precision from CNN using whole data as test: {test_precision_score*100}%\")\n",
    "\n",
    "test_recall_score = recall_score(Y_val_labels, Y_pred_labels)\n",
    "test_recall_score_list.append(test_recall_score)\n",
    "print(f\"Test Recall from CNN using whole data as test: {test_recall_score*100}%\")\n",
    "\n",
    "test_f1_score = f1_score(Y_val_labels, Y_pred_labels)\n",
    "test_f1_score_list.append(test_f1_score)\n",
    "print(f\"Test F1 Score from CNN using whole data as test: {test_f1_score*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "\n",
    "Model_Frames = \"CNN Model with middle 50% frames skip 1 grey\"\n",
    "Model_Frames_list.append(Model_Frames)\n",
    "\n",
    "test_accuracy = accuracy_score(Y_val_labels, Y_pred_labels)\n",
    "test_accuracy_list.append(test_accuracy)\n",
    "print(f\"Test Accuracy from CNN using whole data as test: {test_accuracy*100}%\")\n",
    "\n",
    "test_precision_score = precision_score(Y_val_labels, Y_pred_labels)\n",
    "test_precision_score_list.append(test_precision_score)\n",
    "print(f\"Test Precision from CNN using whole data as test: {test_precision_score*100}%\")\n",
    "\n",
    "test_recall_score = recall_score(Y_val_labels, Y_pred_labels)\n",
    "test_recall_score_list.append(test_recall_score)\n",
    "print(f\"Test Recall from CNN using whole data as test: {test_recall_score*100}%\")\n",
    "\n",
    "test_f1_score = f1_score(Y_val_labels, Y_pred_labels)\n",
    "test_f1_score_list.append(test_f1_score)\n",
    "print(f\"Test F1 Score from CNN using whole data as test: {test_f1_score*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "\n",
    "Model_Frames = \"CNN Model with middle 50% frames skip 2 grey\"\n",
    "Model_Frames_list.append(Model_Frames)\n",
    "\n",
    "test_accuracy = accuracy_score(Y_val_labels, Y_pred_labels)\n",
    "test_accuracy_list.append(test_accuracy)\n",
    "print(f\"Test Accuracy from CNN using whole data as test: {test_accuracy*100}%\")\n",
    "\n",
    "test_precision_score = precision_score(Y_val_labels, Y_pred_labels)\n",
    "test_precision_score_list.append(test_precision_score)\n",
    "print(f\"Test Precision from CNN using whole data as test: {test_precision_score*100}%\")\n",
    "\n",
    "test_recall_score = recall_score(Y_val_labels, Y_pred_labels)\n",
    "test_recall_score_list.append(test_recall_score)\n",
    "print(f\"Test Recall from CNN using whole data as test: {test_recall_score*100}%\")\n",
    "\n",
    "test_f1_score = f1_score(Y_val_labels, Y_pred_labels)\n",
    "test_f1_score_list.append(test_f1_score)\n",
    "print(f\"Test F1 Score from CNN using whole data as test: {test_f1_score*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30e9c8",
   "metadata": {},
   "source": [
    "## Calculating metrics at Clip level for CNN + DenseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_val = pd.DataFrame()\n",
    "\n",
    "output_val['Predicted_CNN'] = Y_pred_labels\n",
    "output_val['DiseaseID'] = Y_val_labels\n",
    "output_val['Error_val'] = abs(output_val['DiseaseID'] - output_val['Predicted_CNN'])\n",
    "\n",
    "output_val['Paciente '] = train['Paciente '].values[train_len:]\n",
    "output_val['Video ID'] = train['Video ID'].values[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6dac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating at Clip level\n",
    "error_table = output_val[['Paciente ','Video ID','DiseaseID','Error_val']].groupby(['Paciente ','Video ID']).sum().reset_index()\n",
    "count_rows = output_val[['Paciente ','Video ID','DiseaseID']].groupby(['Paciente ','Video ID']).count().reset_index().rename(columns = {'DiseaseID':'Count'})\n",
    "error_table = pd.merge(error_table, count_rows, how = 'inner', on = ['Paciente ','Video ID'])\n",
    "error_table['Error_percent'] = error_table['Error_val']/error_table['Count']\n",
    "\n",
    "error_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef7f39-2043-432c-bae9-243b62b9eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Frames_tag_list = []\n",
    "accuracy_list = []\n",
    "precision_score_list = []\n",
    "recall_score_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "# middle 50% frames skip 2 grey\n",
    "FN = len(error_table[(error_table['Error_percent'] >= 0.5)&(error_table['DiseaseID'] > 0)])\n",
    "FP = len(error_table[(error_table['Error_percent'] > 0.5)&(error_table['DiseaseID'] == 0)])\n",
    "TN = len(error_table[(error_table['Error_percent'] <= 0.5)&(error_table['DiseaseID'] == 0)])\n",
    "TP = len(error_table[(error_table['Error_percent'] < 0.5)&(error_table['DiseaseID'] > 0)])\n",
    "\n",
    "Model_Frames = \"CNN Model with middle 50% frames skip 2 grey\"\n",
    "Model_Frames_tag_list.append(Model_Frames)\n",
    "\n",
    "Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "accuracy_list.append(Accuracy)\n",
    "\n",
    "Precision = TP/(TP+FP)\n",
    "precision_score_list.append(Precision)\n",
    "\n",
    "Recall = TP/(TP+FN)\n",
    "recall_score_list.append(Recall)\n",
    "\n",
    "F1_Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "f1_score_list.append(F1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Model info':Model_Frames_tag_list, 'Accuracy':accuracy_list, 'Precision':precision_score_list, 'Recall':recall_score_list, 'F1 Score':f1_score_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda66636-3d5d-419f-b4d1-88f4b479377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Frames_tag_list = []\n",
    "accuracy_list = []\n",
    "precision_score_list = []\n",
    "recall_score_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "# middle 50% frames skip 1 grey\n",
    "FN = len(error_table[(error_table['Error_percent'] >= 0.5)&(error_table['DiseaseID'] > 0)])\n",
    "FP = len(error_table[(error_table['Error_percent'] > 0.5)&(error_table['DiseaseID'] == 0)])\n",
    "TN = len(error_table[(error_table['Error_percent'] <= 0.5)&(error_table['DiseaseID'] == 0)])\n",
    "TP = len(error_table[(error_table['Error_percent'] < 0.5)&(error_table['DiseaseID'] > 0)])\n",
    "\n",
    "Model_Frames = \"CNN Model with middle 50% frames skip 1 grey\"\n",
    "Model_Frames_tag_list.append(Model_Frames)\n",
    "\n",
    "Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "accuracy_list.append(Accuracy)\n",
    "\n",
    "Precision = TP/(TP+FP)\n",
    "precision_score_list.append(Precision)\n",
    "\n",
    "Recall = TP/(TP+FN)\n",
    "recall_score_list.append(Recall)\n",
    "\n",
    "F1_Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "f1_score_list.append(F1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Model info':Model_Frames_tag_list, 'Accuracy':accuracy_list, 'Precision':precision_score_list, 'Recall':recall_score_list, 'F1 Score':f1_score_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Frames_tag_list = []\n",
    "accuracy_list = []\n",
    "precision_score_list = []\n",
    "recall_score_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "# middle 50% frames skip 0 grey\n",
    "FN = len(error_table[(error_table['Error_percent'] >= 0.5)&(error_table['DiseaseID'] > 0)])\n",
    "FP = len(error_table[(error_table['Error_percent'] > 0.5)&(error_table['DiseaseID'] == 0)])\n",
    "TN = len(error_table[(error_table['Error_percent'] <= 0.5)&(error_table['DiseaseID'] == 0)])\n",
    "TP = len(error_table[(error_table['Error_percent'] < 0.5)&(error_table['DiseaseID'] > 0)])\n",
    "\n",
    "Model_Frames = \"CNN Model with middle 50% frames skip 0 grey\"\n",
    "Model_Frames_tag_list.append(Model_Frames)\n",
    "\n",
    "Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "accuracy_list.append(Accuracy)\n",
    "\n",
    "Precision = TP/(TP+FP)\n",
    "precision_score_list.append(Precision)\n",
    "\n",
    "Recall = TP/(TP+FN)\n",
    "recall_score_list.append(Recall)\n",
    "\n",
    "F1_Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "f1_score_list.append(F1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Model info':Model_Frames_tag_list, 'Accuracy':accuracy_list, 'Precision':precision_score_list, 'Recall':recall_score_list, 'F1 Score':f1_score_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d98e89",
   "metadata": {},
   "source": [
    "## Getting the 64 length array from second last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: try this code, if this gives error or takes too long try the code below\n",
    "import time\n",
    "start = time.time()\n",
    "from keras import backend\n",
    "\n",
    "feature_df_file = \"Feature_dataframes/Feature_CNN_16batch_middle50skip0_grey.csv\"\n",
    "\n",
    "colnames = []\n",
    "# creating column name list\n",
    "for i in range(64):\n",
    "    colnames.append(\"col_\" + str(i))\n",
    "    \n",
    "index_for_classif = []\n",
    "X = pd.DataFrame(columns = colnames)\n",
    "\n",
    "count = 0\n",
    "\n",
    "layer_name = model.layers[-2].name #for getting second last layer name\n",
    "\n",
    "# the extraction happens for each Patient at a time.\n",
    "for patient in train['Paciente '].unique():\n",
    "    count+=1\n",
    "    index_for_classif = list(train[train['Paciente ']==patient].index)\n",
    "    \n",
    "    print(len(index_for_classif))\n",
    "    \n",
    "    X_New = X_Train[index_for_classif]\n",
    "    specific_layer_output = backend.function([model.layers[0].input], [model.get_layer(layer_name).output]) \n",
    "    layer_output = specific_layer_output([X_New])[0]\n",
    "    \n",
    "    df = pd.DataFrame(layer_output ,columns = colnames)\n",
    "    X = pd.concat([X,df])\n",
    "\n",
    "    if count == 5:\n",
    "        X = X.reset_index(drop = True)\n",
    "        X.to_csv(feature_df_file, index = False)\n",
    "\n",
    "        X = pd.DataFrame(columns = colnames)\n",
    "    elif count%5 == 0:\n",
    "        X = X.reset_index(drop = True)\n",
    "        # Save the updated DataFrame to the CSV file\n",
    "        X.to_csv(feature_df_file, mode='a', header=False, index=False)\n",
    "\n",
    "        X = pd.DataFrame(columns = colnames)\n",
    "\n",
    "if count%5!=0:\n",
    "    X = X.reset_index(drop = True)\n",
    "    # Save the updated DataFrame to the CSV file\n",
    "    X.to_csv(feature_df_file, mode='a', header=False, index=False)\n",
    "\n",
    "    X = pd.DataFrame(columns = colnames)\n",
    "\n",
    "print((time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfa8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: if the code above doesn't work, run this      \n",
    "import time\n",
    "start = time.time()\n",
    "from keras import backend\n",
    "\n",
    "feature_df_file = \"Feature_dataframes/Feature_CNN_16batch_middle50skip0_grey.csv\"\n",
    "\n",
    "colnames = []\n",
    "# creating column name list\n",
    "for i in range(64):\n",
    "    colnames.append(\"col_\" + str(i))\n",
    "    \n",
    "index_for_classif = []\n",
    "X = pd.DataFrame(columns = colnames)\n",
    "\n",
    "count = 0\n",
    "\n",
    "layer_name = model.layers[-2].name #for getting second last layer name\n",
    "\n",
    "# for skip 0 middle 50%, the extraction will happen for each Patient-VideoID combination at a time. This is to reduce the memory consumption\n",
    "for key in train['key'].unique():\n",
    "    count+=1\n",
    "    index_for_classif = list(train[train['key']==key].index)\n",
    "    \n",
    "    print(len(index_for_classif))\n",
    "    \n",
    "    X_New = X_Train[index_for_classif]\n",
    "    specific_layer_output = backend.function([model.layers[0].input], [model.get_layer(layer_name).output])\n",
    "    layer_output = specific_layer_output([X_New])[0]\n",
    "    \n",
    "    df = pd.DataFrame(layer_output ,columns = colnames)\n",
    "    X = pd.concat([X,df])\n",
    "\n",
    "    if count == 2:\n",
    "        X = X.reset_index(drop = True)\n",
    "        X.to_csv(feature_df_file, index = False)\n",
    "\n",
    "        X = pd.DataFrame(columns = colnames)\n",
    "    elif count%2 == 0:\n",
    "        X = X.reset_index(drop = True)\n",
    "        # Save the updated DataFrame to the CSV file\n",
    "        X.to_csv(feature_df_file, mode='a', header=False, index=False)\n",
    "\n",
    "        X = pd.DataFrame(columns = colnames)\n",
    "\n",
    "if count%2!=0:\n",
    "    X = X.reset_index(drop = True)\n",
    "    # Save the updated DataFrame to the CSV file\n",
    "    X.to_csv(feature_df_file, mode='a', header=False, index=False)\n",
    "\n",
    "    X = pd.DataFrame(columns = colnames)\n",
    "\n",
    "print((time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b123d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: if the code above doesn't work, run this      \n",
    "import time\n",
    "start = time.time()\n",
    "from keras import backend\n",
    "\n",
    "feature_df_file = \"Feature_dataframes/Feature_CNN_16batch_middle50skip0_grey.csv\"\n",
    "\n",
    "colnames = []\n",
    "# creating column name list\n",
    "for i in range(64):\n",
    "    colnames.append(\"col_\" + str(i))\n",
    "    \n",
    "index_for_classif = []\n",
    "X = pd.DataFrame(columns = colnames)\n",
    "\n",
    "count = 0\n",
    "\n",
    "layer_name = model.layers[-2].name #for getting second last layer name\n",
    "\n",
    "# for skip 0 middle 50%, the extraction will happen for each Patient-VideoID combination at a time. This is to reduce the memory consumption\n",
    "for key in train['key'].unique():\n",
    "    count+=1\n",
    "    index_for_classif = list(train[train['key']==key].index)\n",
    "    \n",
    "    print(len(index_for_classif))\n",
    "    \n",
    "    X_New = X_Train[index_for_classif]\n",
    "    specific_layer_output = backend.function([model.layers[0].input], [model.get_layer(layer_name).output])\n",
    "    layer_output = specific_layer_output([X_New])[0]\n",
    "    \n",
    "    df = pd.DataFrame(layer_output ,columns = colnames)\n",
    "    X = pd.concat([X,df])\n",
    "\n",
    "    if count == 1:\n",
    "        X = X.reset_index(drop = True)\n",
    "        X.to_csv(feature_df_file, index = False)\n",
    "\n",
    "        X = pd.DataFrame(columns = colnames)\n",
    "    else:\n",
    "        X = X.reset_index(drop = True)\n",
    "        # Save the updated DataFrame to the CSV file\n",
    "        X.to_csv(feature_df_file, mode='a', header=False, index=False)\n",
    "\n",
    "        X = pd.DataFrame(columns = colnames)\n",
    "\n",
    "print((time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a824316",
   "metadata": {},
   "source": [
    "## ML Classifier modelling at Frame level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving File\n",
    "\n",
    "# # Middle 50%\n",
    "# X.to_csv(\"Feature_dataframes/Feature_CNN_16batch_middle50skip2_grey.csv\", index = False) # middle 50% skip 2 \n",
    "# X.to_csv(\"Feature_dataframes/Feature_CNN_16batch_middle50skip1_grey.csv\", index = False) # middle 50% skip 1\n",
    "# X.to_csv(\"Feature_dataframes/Feature_CNN_16batch_middle50skip0_grey.csv\", index = False) # middle 50% skip 0\n",
    "\n",
    "# reading the feature csv\n",
    "X = pd.read_csv(\"Feature_dataframes/Feature_CNN_16batch_middle50skip0_grey.csv\") \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e235810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['DiseaseID'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train and test\n",
    "X_training = X[:train_len]\n",
    "X_testing = X[train_len:]\n",
    "y_training = y[:train_len]\n",
    "y_testing = y[train_len:]\n",
    "\n",
    "X_training = X_training.reset_index(drop = True)\n",
    "X_testing = X_testing.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fab054-81f6-4529-8ac3-43b658f58d66",
   "metadata": {},
   "source": [
    "#### Modeling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c383f-dd4b-42b3-a10a-1fae9807ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "dict_values_cv = {'Algorithm' : [], 'Parameters': [], 'Accuracy':[], 'F1':[], 'Precision':[], 'Recall':[] }\n",
    "\n",
    "# 1. Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "model = LogisticRegression(random_state=SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"Logistic Regression\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"Logistic Regression done!\")\n",
    "\n",
    "# 2. Guassian NB\n",
    "param_grid = {}\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"Gaussian Naive Bayes\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"Gaussian NB done!\")\n",
    "\n",
    "# 3. SVC\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "model = SVC(random_state=SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"SVM Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"SVC done!\")\n",
    "\n",
    "# 3. Random Forest Classifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30, 50]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"Random Forest Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"Random Forest done!\")\n",
    "\n",
    "# 4. Bagging Classifier\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_samples': [0.5, 1.0],\n",
    "    'max_features': [0.5, 1.0]\n",
    "}\n",
    "\n",
    "model = BaggingClassifier(estimator=SVC(kernel=\"linear\"), random_state= SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"SVC Bagging Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"SVC Bagging done!\")\n",
    "\n",
    "# 5. KNN Classifier\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 10, 20, 30],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"K-Nearest Neighbors Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"KNN done!\")\n",
    "\n",
    "# 5. Decision Tree Classifier\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 20]\n",
    "}\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"Decision Tree Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"Decision Tree done!\")\n",
    "\n",
    "print((time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c57035-758c-44da-b7ad-f4187318e408",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc744e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(dict_values_cv).to_markdown()) # middle50%_skip0_grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0db9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(dict_values_cv).to_markdown()) # middle50%_skip1_grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd246aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(dict_values_cv).to_markdown()) # middle50%_skip2_grey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66b1ac",
   "metadata": {},
   "source": [
    "## ML Classifier modelling at Clip level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving File\n",
    "# # Middle 50%\n",
    "# X.to_csv(\"Feature_dataframes/Feature_CNN_16batch_middle50skip2_grey.csv\", index = False) # middle 50% skip 2 \n",
    "# X.to_csv(\"Feature_dataframes/Feature_CNN_16batch_middle50skip1_grey.csv\", index = False) # middle 50% skip 1\n",
    "# X.to_csv(\"Feature_dataframes/Feature_CNN_16batch_middle50skip0_grey.csv\", index = False) # middle 50% skip 0\n",
    "\n",
    "# read feature csv\n",
    "X = pd.read_csv(\"Feature_dataframes/Feature_CNN_16batch_middle50skip0_grey.csv\")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104848d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X, train[['Paciente ', 'Video ID']]], axis = 1)\n",
    "\n",
    "y = train[['Paciente ', 'Video ID', 'DiseaseID']]\n",
    "\n",
    "# splitting train and test\n",
    "X_training = X[:train_len]\n",
    "X_testing = X[train_len:]\n",
    "y_training = y[:train_len]\n",
    "y_testing = y[train_len:]\n",
    "\n",
    "X_training = X_training.reset_index(drop = True)\n",
    "X_testing = X_testing.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac377f0",
   "metadata": {},
   "source": [
    "#### Averaging out the feature values at Clip level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = X_training.groupby(by = ['Paciente ', 'Video ID']).mean().reset_index()\n",
    "X_training = X_training.sort_values(by = ['Paciente ','Video ID']).reset_index(drop = True)\n",
    "key_training = X_training[['Paciente ', 'Video ID']]\n",
    "print(X_training[X_training['Paciente '] == \"R11\"])\n",
    "X_training = X_training.drop(['Paciente ', 'Video ID'], axis = 1)\n",
    " \n",
    "\n",
    "y_training = y_training[['Paciente ', 'Video ID', 'DiseaseID']].drop_duplicates().reset_index(drop = True)\n",
    "y_training = y_training.sort_values(by = ['Paciente ','Video ID']).reset_index(drop = True)\n",
    "print(y_training[y_training['Paciente '] == \"R11\"])\n",
    "y_training = y_training['DiseaseID'].values\n",
    "\n",
    "X_testing = X_testing.groupby(by = ['Paciente ', 'Video ID']).mean().reset_index()\n",
    "X_testing = X_testing.sort_values(by = ['Paciente ','Video ID']).reset_index(drop = True)\n",
    "key_testing = X_testing[['Paciente ', 'Video ID']]\n",
    "print(X_testing[X_testing['Paciente '] == \"R11\"])\n",
    "X_testing = X_testing.drop(['Paciente ', 'Video ID'], axis = 1)\n",
    "\n",
    "y_testing = y_testing[['Paciente ', 'Video ID', 'DiseaseID']].drop_duplicates().reset_index(drop = True)\n",
    "y_testing = y_testing.sort_values(by = ['Paciente ','Video ID']).reset_index(drop = True)\n",
    "print(y_testing[y_testing['Paciente '] == \"R11\"])\n",
    "y_testing = y_testing['DiseaseID'].values\n",
    "\n",
    "X = pd.concat([X_training, X_testing]).reset_index(drop = True)\n",
    "y = np.append(y_training, y_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c7287d-aaff-4304-a33a-57e7b463667e",
   "metadata": {},
   "source": [
    "#### Modeling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0d3a6-31e3-419c-b39c-50fc9d822154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "dict_values_cv = {'Algorithm' : [], 'Parameters': [], 'Accuracy':[], 'F1':[], 'Precision':[], 'Recall':[] }\n",
    "\n",
    "# 1. Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "model = LogisticRegression(random_state=SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"Logistic Regression\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"Logistic Regression done!\")\n",
    "\n",
    "# 2. Guassian NB\n",
    "param_grid = {}\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"Gaussian Naive Bayes\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"Gaussian NB done!\")\n",
    "\n",
    "# 3. SVC\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "model = SVC(random_state=SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"SVM Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"SVC done!\")\n",
    "\n",
    "# 3. Random Forest Classifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30, 50]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"Random Forest Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"Random Forest done!\")\n",
    "\n",
    "# 4. Bagging Classifier\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_samples': [0.5, 1.0],\n",
    "    'max_features': [0.5, 1.0]\n",
    "}\n",
    "\n",
    "model = BaggingClassifier(estimator=SVC(kernel=\"linear\"), random_state= SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"SVC Bagging Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"SVC Bagging done!\")\n",
    "\n",
    "# 5. KNN Classifier\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 10, 20, 30],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"K-Nearest Neighbors Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"KNN done!\")\n",
    "\n",
    "# 5. Decision Tree Classifier\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 20]\n",
    "}\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_training, y_training)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "best_estimator.fit(X_training, y_training) # retrain on the training set\n",
    "\n",
    "y_pred = best_estimator.predict(X_testing)\n",
    "\n",
    "dict_values_cv['Algorithm'].append(\"Decision Tree Classifier\")\n",
    "dict_values_cv['Parameters'].append(grid_search.best_params_)\n",
    "dict_values_cv['Accuracy'].append(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "dict_values_cv['F1'].append(np.round(f1_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Precision'].append(np.round(precision_score(y_testing, y_pred, average='binary'),4))\n",
    "dict_values_cv['Recall'].append(np.round(recall_score(y_testing, y_pred, average='binary'),4))\n",
    "\n",
    "print(\"Decision Tree done!\")\n",
    "\n",
    "print((time.time() - start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704a59f-b063-4766-acd5-4913202cee43",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcaba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(dict_values_cv).to_markdown()) # middle50%_skip0_grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9942200",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(dict_values_cv).to_markdown()) # middle50%_skip1_grey "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(dict_values_cv).to_markdown()) # middle50%_skip2_grey "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07b2f4",
   "metadata": {},
   "source": [
    "## For the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = best_estimator.predict(X_training)\n",
    "y_pred_all = best_estimator.predict(X)\n",
    "y_pred_proba_all = best_estimator.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(accuracy_score(y_training, y_pred_train),4))\n",
    "print(len(y_training))\n",
    "print(y_training!=y_pred_train)\n",
    "print(np.round(accuracy_score(y_testing, y_pred),4))\n",
    "print(y_testing!=y_pred)\n",
    "print(len(y_testing))\n",
    "\n",
    "print(np.append(y_training,y_testing) != y)\n",
    "print(np.append(y_pred_train,y_pred) != y_pred_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(accuracy_score(y, y_pred_all),4))\n",
    "print(np.round(precision_score(y, y_pred_all, average='binary'),4))\n",
    "print(np.round(recall_score(y, y_pred_all, average='binary'),4))\n",
    "print(np.round(f1_score(y, y_pred_all, average='binary'),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_training['Split'] = 'train'\n",
    "key_testing['Split'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predictions = pd.concat([key_training, key_testing]).reset_index(drop = True)\n",
    "output_predictions['DiseaseID'] = y\n",
    "output_predictions['Predictions'] = y_pred_all\n",
    "output_predictions['Probability/Confidence of the prediction'] = np.max(y_pred_proba_all, axis=1)\n",
    "\n",
    "print(output_predictions)\n",
    "print(output_predictions[output_predictions['Paciente '] == 'R11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d39fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predictions.to_excel('Prediction_output_with_confidence.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
